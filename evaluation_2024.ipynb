{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Output\n",
    "## Detection\n",
    "Output of the detection module of a volume:\n",
    "\n",
    "```\n",
    "[\n",
    "  [[zs1, ys1, xs1, ze1, ye1, xe1], obj_score1, class1_score1, class2_score1],\n",
    "  [[zs2, ys2, xs2, ze2, ye2, xe2], obj_score2, class1_score2, class2_score2],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "In the format above, xs, ys, and zs represents starts of the bounding box, xe, ye and ze represents ends of the bounding box. \"obj_score\" represents the confidence of this bounding box, \"class1_score\" represents the probability of this bounding box as the first class (intracranial aneurysm in this challenge), and \"class2_score\" represents the probability of this bounding box as the second class (stenosis). Note that the sum of probabilities of two classes should be 1.\n",
    "\n",
    "When evaluating each class of detection, the output will be processed using the following code to adapt the detection metrics code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[([10, 10, 10, 80, 80, 80], 0.5)], [([20, 20, 20, 70, 70, 70], 0.4)]]\n",
      "[[([50, 50, 50, 150, 150, 150], 0.6)], [([60, 60, 60, 120, 120, 120], 0.1)]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def convert_output_to_detection(output, wanted_class_num=1):\n",
    "    \"\"\"\n",
    "    Convert the output of the model to a list of detections.\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "    for i in range(len(output)):  # detections of each image\n",
    "        lst_this_image = []\n",
    "        for j in range(len(output[i])):  # detections in this image\n",
    "            lst2 = output[i][j][2:]\n",
    "            max_idx = np.argmax(lst2)\n",
    "            if max_idx + 1 == wanted_class_num:\n",
    "                lst_this_image.append((output[i][j][0], output[i][j][1]))\n",
    "        detections.append(lst_this_image)\n",
    "    return detections\n",
    "\n",
    "output = [\n",
    "    [[[50, 50, 50, 150, 150, 150], 0.6, 0.1, 0.9], [[10, 10, 10, 80, 80, 80], 0.5, 0.8, 0.2]],  # Prediction for image 1\n",
    "    [[[20, 20, 20, 70, 70, 70], 0.4, 0.7, 0.3], [[60, 60, 60, 120, 120, 120], 0.1, 0.4, 0.6]],  # Prediction for image 2\n",
    "]\n",
    "\n",
    "print(convert_output_to_detection(output, 1))\n",
    "print(convert_output_to_detection(output, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "Output of the segmentation task should be **an array with the same size as the input image**, with label 1 as segmented lesion (no matter whether intracranial aneurysm or stenosis). Patches in successfully detected bounding boxes will be cropped and segmentation metrics and clinical metrics will be calculated within the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Part\n",
    "Definitions of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iou_3d(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two 3D bounding boxes.\n",
    "    box1 and box2 should be in the format [x1, y1, z1, x2, y2, z2]\n",
    "    \"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    z1_inter = max(box1[2], box2[2])\n",
    "    x2_inter = min(box1[3], box2[3])\n",
    "    y2_inter = min(box1[4], box2[4])\n",
    "    z2_inter = min(box1[5], box2[5])\n",
    "\n",
    "    if x1_inter < x2_inter and y1_inter < y2_inter and z1_inter < z2_inter:\n",
    "        inter_volume = (x2_inter - x1_inter) * (y2_inter - y1_inter) * (z2_inter - z1_inter)\n",
    "    else:\n",
    "        inter_volume = 0\n",
    "\n",
    "    box1_volume = (box1[3] - box1[0]) * (box1[4] - box1[1]) * (box1[5] - box1[2])\n",
    "    box2_volume = (box2[3] - box2[0]) * (box2[4] - box2[1]) * (box2[5] - box2[2])\n",
    "\n",
    "    iou_value = inter_volume / (box1_volume + box2_volume - inter_volume)\n",
    "    return iou_value\n",
    "\n",
    "def compute_precision_recall(pred_boxes_lst, gt_boxes_lst, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    :param pred_boxes_lst: list of instances, each instance is a list of predicted boxes, each box is a tuple of [bbox, confidence]. \n",
    "    e.g. [[([x1, y1, z1, x2, y2, z2], confidence), ...], ...]\n",
    "    :param gt_boxes_lst: list of instances, each instance is a list of ground truth boxes\n",
    "    e.g. [[[x1, y1, z1, x2, y2, z2], ...], ...]\n",
    "    \"\"\"\n",
    "    all_tp = []\n",
    "    all_fp = []\n",
    "    all_probs = []\n",
    "    total_gt_boxes = 0\n",
    "    for pred_boxes, gt_boxes in zip(pred_boxes_lst, gt_boxes_lst):\n",
    "        \n",
    "        pred_boxes = sorted(pred_boxes, key=lambda x: x[1], reverse=True)\n",
    "        tp = np.zeros(len(pred_boxes))\n",
    "        fp = np.zeros(len(pred_boxes))\n",
    "        probs = np.zeros(len(pred_boxes))\n",
    "        total_gt_boxes_this = len(gt_boxes)\n",
    "        total_gt_boxes += total_gt_boxes_this\n",
    "\n",
    "        matched_gt = []\n",
    "\n",
    "        for pred_idx, pred in enumerate(pred_boxes):\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            for gt_idx, gt in enumerate(gt_boxes):\n",
    "                if gt_idx in matched_gt:\n",
    "                    continue\n",
    "                iou_value = iou_3d(pred[0], gt)\n",
    "                if iou_value > best_iou:\n",
    "                    best_iou = iou_value\n",
    "                    best_gt_idx = gt_idx\n",
    "\n",
    "            if best_iou >= iou_threshold:\n",
    "                tp[pred_idx] = 1\n",
    "                matched_gt.append(best_gt_idx)\n",
    "            else:\n",
    "                fp[pred_idx] = 1\n",
    "            probs[pred_idx] = pred[1]\n",
    "\n",
    "        all_tp.append(tp)\n",
    "        all_fp.append(fp)\n",
    "        all_probs.append(probs)\n",
    "    \n",
    "    tp = np.concatenate(all_tp)\n",
    "    fp = np.concatenate(all_fp)\n",
    "    probs = np.concatenate(all_probs)\n",
    "\n",
    "    sorted_indices = np.argsort(-probs)\n",
    "    tp = tp[sorted_indices]\n",
    "    fp = fp[sorted_indices]\n",
    "    probs = probs[sorted_indices]\n",
    "\n",
    "    cumulative_tp = np.cumsum(tp)\n",
    "    cumulative_fp = np.cumsum(fp)\n",
    "    precision = cumulative_tp / (cumulative_tp + cumulative_fp)\n",
    "    recall = cumulative_tp / total_gt_boxes\n",
    "\n",
    "    return precision, recall, probs\n",
    "\n",
    "def compute_ap(precision, recall, eleven_points_avg=False):\n",
    "    \"\"\"\n",
    "    Compute Average Precision (AP) given precision and recall arrays\n",
    "    \"\"\"\n",
    "    precision = np.concatenate(([1.0], precision, [0.0]))\n",
    "    recall = np.concatenate(([0.0], recall, [1.0]))\n",
    "\n",
    "    for i in range(len(precision) - 1, 0, -1):  # Smoothing the precision curve\n",
    "        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n",
    "\n",
    "    # Compute the area under the curve\n",
    "    if not eleven_points_avg:\n",
    "        indices = np.where(recall[1:] != recall[:-1])[0]\n",
    "        ap = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])\n",
    "    else:\n",
    "        # Compute average precision at 11 recall points\n",
    "        recall_points = np.linspace(0, 1, 11)\n",
    "        ap = 0\n",
    "        for recall_point in recall_points:\n",
    "            if np.sum(recall >= recall_point) == 0:\n",
    "                precision_point = 0\n",
    "            else:\n",
    "                precision_point = np.max(precision[recall >= recall_point])\n",
    "            ap += precision_point\n",
    "        ap /= 11\n",
    "\n",
    "    return ap\n",
    "\n",
    "def calculate_ap(predictions, ground_truths, iou_threshold=0.5, eleven_points_avg=False):\n",
    "    \"\"\"\n",
    "    Calculate AP50 from prediction and ground truth boxes for multiple instances\n",
    "    predictions: list of list of predictions per image, each prediction is a list of [bbox, confidence]\n",
    "    ground_truths: list of list of ground truth boxes per image\n",
    "    \"\"\"\n",
    "    all_precisions, all_recalls, _ = compute_precision_recall(predictions, ground_truths, iou_threshold)\n",
    "    if len(all_precisions) == 0:\n",
    "        return 0\n",
    "\n",
    "    return compute_ap(all_precisions, all_recalls, eleven_points_avg=eleven_points_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: 0.7803030303030303\n"
     ]
    }
   ],
   "source": [
    "predictions_IA = [  # Predictions for each image in the ENTIRE test set / validation set / training set of ONE class, e.g. aneurysm\n",
    "    [([50, 50, 50, 150, 150, 150], 0.6), ([10, 10, 10, 80, 80, 80], 0.5)],  # Prediction for image 1\n",
    "    [([20, 20, 20, 70, 70, 70], 0.4), ([60, 60, 60, 120, 120, 120], 0.1)],  # Prediction for image 2\n",
    "    [([40, 40, 40, 160, 160, 160], 0.7), ([35, 35, 35, 105, 105, 105], 0.2)],  # Prediction for image 3\n",
    "]\n",
    "ground_truths_IA = [  # Ground truths for each image in the ENTIRE test set / validation set / training set of ONE class, e.g. aneurysm\n",
    "    [[40, 40, 40, 160, 160, 160], [35, 35, 35, 105, 105, 105]],  # Ground truth for image 1\n",
    "    [[15, 15, 15, 75, 75, 75], [65, 65, 65, 130, 130, 130]],  # Ground truth for image 2\n",
    "    [[50, 50, 50, 150, 150, 150], [10, 10, 10, 80, 80, 80]],  # Ground truth for image 3\n",
    "]\n",
    "\n",
    "ap15 = calculate_ap(predictions_IA, ground_truths_IA, iou_threshold=0.15, eleven_points_avg=True)\n",
    "ap25 = calculate_ap(predictions_IA, ground_truths_IA, iou_threshold=0.25, eleven_points_avg=True)\n",
    "ap_IA = (ap15 + ap25) / 2\n",
    "print(f'AP: {ap_IA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Part\n",
    "Definition of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from medpy.metric.binary import hd95\n",
    "\n",
    "def dice_score(pred, gt):\n",
    "    \"\"\"\n",
    "    Calculate Dice score between two binary masks\n",
    "    \"\"\"\n",
    "    pred = np.bool_(pred)\n",
    "    gt = np.bool_(gt)\n",
    "    intersection = np.count_nonzero(pred & gt)\n",
    "    union = np.count_nonzero(pred | gt)\n",
    "    dice = 2 * intersection / (np.count_nonzero(pred) + np.count_nonzero(gt))\n",
    "    return dice\n",
    "\n",
    "def hausdorff_distance_unified(pred, gt, baseline, voxel_spacing):\n",
    "    \"\"\"\n",
    "    Calculate Hausdorff distance between two binary masks, then unify the result to (0-1) with a baseline\n",
    "    \"\"\"\n",
    "    pred = np.bool_(pred)\n",
    "    gt = np.bool_(gt)\n",
    "    hd = hd95(pred, gt, voxel_spacing)\n",
    "    hd_baseline = hd95(baseline, gt, voxel_spacing)\n",
    "    hd = 1 - hd / hd_baseline\n",
    "    if hd < 0:\n",
    "        hd = 0\n",
    "    return hd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice: 0.5673352435530086, HD: 0.38350379760491016\n"
     ]
    }
   ],
   "source": [
    "# each metric of one type of lesion is calculated for each lesion (i.e. each bounding box), then averaged across all lesions\n",
    "dice_scores = []\n",
    "hd_scores = []\n",
    "\n",
    "# Example: 160*200*200 image, spacing is (0.8, 0.6, 0.6), lesion ground-truth bounding box at [x1, y1, z1, x2, y2, z2] = [30, 30, 30, 50, 50, 50]\n",
    "label_img = np.zeros((160, 200, 200))\n",
    "label_img[30:50, 30:50, 30:50] = 1\n",
    "pred_img = np.zeros((160, 200, 200))\n",
    "pred_img[32:48, 31:49, 34:45] = 1\n",
    "baseline_pred_img = np.zeros((160, 200, 200))  # baseline prediction made by simple thresholding\n",
    "baseline_pred_img[33:47, 32:48, 36:42] = 1\n",
    "spacing = (0.8, 0.6, 0.6)\n",
    "\n",
    "bbox_gt = [30, 30, 30, 50, 50, 50]\n",
    "\n",
    "pred_img_in_bbox = pred_img[bbox_gt[0]:bbox_gt[3], bbox_gt[1]:bbox_gt[4], bbox_gt[2]:bbox_gt[5]]\n",
    "label_img_in_bbox = label_img[bbox_gt[0]:bbox_gt[3], bbox_gt[1]:bbox_gt[4], bbox_gt[2]:bbox_gt[5]]\n",
    "baseline_pred_img_in_bbox = baseline_pred_img[bbox_gt[0]:bbox_gt[3], bbox_gt[1]:bbox_gt[4], bbox_gt[2]:bbox_gt[5]]\n",
    "\n",
    "dice = dice_score(pred_img_in_bbox, label_img_in_bbox)\n",
    "hd = hausdorff_distance_unified(pred_img_in_bbox, label_img_in_bbox, baseline_pred_img_in_bbox, spacing)\n",
    "dice_scores.append(dice)\n",
    "hd_scores.append(hd)\n",
    "\n",
    "total_dice = np.mean(dice_scores)\n",
    "total_hd = np.mean(hd_scores)\n",
    "\n",
    "print(f'Dice: {total_dice}, HD: {total_hd}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Part\n",
    "## Stenosis percentage\n",
    "Definition of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, measure, morphology\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "def max_and_min_diameters(segmentation_image, spacing):\n",
    "    \"\"\"\n",
    "    :param segmentation_image: Binary segmentation image with the vessel as 1 and background as 0.\n",
    "    \"\"\"\n",
    "    binary_image = segmentation_image > 0\n",
    "    skeleton = morphology.skeletonize(binary_image)\n",
    "    distance_transform = distance_transform_edt(binary_image, sampling=spacing)\n",
    "\n",
    "    # Get the coordinates of the skeleton\n",
    "    skeleton_coords = np.column_stack(np.where(skeleton))\n",
    "\n",
    "    # Calculate the diameter at each point in the skeleton\n",
    "    diameters = [2 * distance_transform[tuple(coord)] for coord in skeleton_coords]\n",
    "\n",
    "    return np.max(diameters), np.min(diameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth vessel percentage: 0.45943752238266466, Predicted vessel percentage: 0.6581182706210862, Difference: 0.19868074823842152\n"
     ]
    }
   ],
   "source": [
    "from skimage.morphology import disk\n",
    "\n",
    "def generate_example_image(disk_radii):\n",
    "    image = np.zeros([len(disk_radii), 50, 50])\n",
    "    for i, radius in enumerate(disk_radii):\n",
    "        image[i, 25 - radius:25 + radius + 1, 25 - radius:25 + radius + 1] = disk(radius)\n",
    "\n",
    "    return image\n",
    "\n",
    "spacing = (0.8, 0.6, 0.6)\n",
    "\n",
    "gt_label = generate_example_image([5, 5, 5, 4, 3, 2, 3, 4, 5, 5, 5])  # diameter of ordinary vessel is 9 and stenosis is 5\n",
    "pred_label = generate_example_image([4, 3, 2, 1, 2, 3, 4])  # predicted diameter of ordinary vessel is 9 and stenosis is 3\n",
    "gt_max, gt_min = max_and_min_diameters(gt_label, spacing)  # note that we have labelled the ordinary vessel beside the stenosis site in our test set GT using another label, so we can calculate the diameter of the ordinary vessel\n",
    "_, pred_min = max_and_min_diameters(pred_label, spacing)\n",
    "gt_percentage = (gt_max - gt_min) / gt_max\n",
    "pred_percentage = (gt_max - pred_min) / gt_max\n",
    "print(f'Ground truth vessel percentage: {gt_percentage}, Predicted vessel percentage: {pred_percentage}, Difference: {abs(gt_percentage - pred_percentage)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aneurysm long and short axes length\n",
    "Definition of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage import measure\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "def max_diameter_short_radius(arr):\n",
    "    # 将2D数组转换为1D数组\n",
    "    flat_arr = np.ravel(arr)\n",
    "    # 找到所有值为1的索引\n",
    "    indices = np.where(flat_arr == 1)[0]\n",
    "    # 将索引转换为2D坐标\n",
    "    coordinates = np.column_stack(np.unravel_index(indices, arr.shape))\n",
    "    # 计算所有点之间的距离\n",
    "    distances = squareform(pdist(coordinates))\n",
    "    # 找到距离最远的两个点\n",
    "    i, j = np.unravel_index(np.argmax(distances), distances.shape)\n",
    "    # 计算最大直径\n",
    "    max_diameter = distances[i, j]\n",
    "    # 计算垂直于最大直径的两个点\n",
    "    midpoint = (coordinates[i] + coordinates[j]) / 2\n",
    "    vector = coordinates[j] - coordinates[i]\n",
    "    perp_vector = np.array([-vector[1], vector[0]])\n",
    "    # 计算垂直于最大直径的两个点的坐标\n",
    "    k = np.argmax(np.abs(np.dot(coordinates - midpoint, perp_vector)))\n",
    "    l = np.argmin(np.abs(np.dot(coordinates - midpoint, perp_vector)))\n",
    "    # 计算垂直于最大直径的长度\n",
    "    short_radius = np.linalg.norm(coordinates[k] - coordinates[l])\n",
    "    return max_diameter, short_radius\n",
    "\n",
    "def get_2d_diameters(label_arr,nodule_spacing):\n",
    "    mask = label_arr\n",
    "    largest_z = np.argmax(np.sum(mask, axis=(1,2)))\n",
    "    lag_z = mask[largest_z]\n",
    "    max_diameter, short_diameter =max_diameter_short_radius(lag_z)\n",
    "    return max_diameter*nodule_spacing[1],short_diameter*nodule_spacing[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth max diameter: 6.0, Predicted max diameter: 4.8, Difference: 1.2000000000000002\n",
      "Ground truth short diameter: 4.242640687119285, Predicted short diameter: 3.394112549695428, Difference: 0.8485281374238567\n"
     ]
    }
   ],
   "source": [
    "from skimage.morphology import ball\n",
    "import numpy as np\n",
    "\n",
    "def generate_example_image(disk_radius):\n",
    "    i = np.zeros([100, 100, 100])\n",
    "    b = ball(disk_radius)\n",
    "    i[50:50 + b.shape[0], 50:50 + b.shape[1], 50:50 + b.shape[2]] = b\n",
    "    return i\n",
    "\n",
    "spacing = (0.8, 0.6, 0.6)\n",
    "\n",
    "gt_label = generate_example_image(5)\n",
    "pred_label = generate_example_image(4)\n",
    "bbox = [50, 50, 50, 50 + 2 * 5 + 1, 50 + 2 * 5 + 1, 50 + 2 * 5 + 1]\n",
    "\n",
    "gt_label_in_bbox = gt_label[bbox[0]:bbox[3], bbox[1]:bbox[4], bbox[2]:bbox[5]]\n",
    "pred_label_in_bbox = pred_label[bbox[0]:bbox[3], bbox[1]:bbox[4], bbox[2]:bbox[5]]\n",
    "\n",
    "gt_max_diameter, gt_short_diameter = get_2d_diameters(gt_label, spacing)\n",
    "pred_max_diameter, pred_short_diameter = get_2d_diameters(pred_label, spacing)\n",
    "print(f'Ground truth max diameter: {gt_max_diameter}, Predicted max diameter: {pred_max_diameter}, Difference: {abs(gt_max_diameter - pred_max_diameter)}')\n",
    "print(f'Ground truth short diameter: {gt_short_diameter}, Predicted short diameter: {pred_short_diameter}, Difference: {abs(gt_short_diameter - pred_short_diameter)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
